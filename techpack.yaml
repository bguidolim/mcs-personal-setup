schemaVersion: 1
identifier: mcs-continuous-learning
displayName: "Continuous Learning"
description: Persistent memory and knowledge management for Claude Code
version: "1.0.0"
author: Bruno Guidolim
minMCSVersion: "2026.2.28"

# ---------------------------------------------------------------------------
# Prompts — resolved interactively during `mcs configure`
# ---------------------------------------------------------------------------
prompts:
  - key: BRANCH_PREFIX
    type: input
    label: "Branch prefix (e.g. feature) -> feature/add-login-screen"
    default: "feature"

# ---------------------------------------------------------------------------
# Components
# ---------------------------------------------------------------------------
components:
  # ── Dependencies ────────────────────────────────────────────────────────
  - id: homebrew
    displayName: Homebrew
    description: macOS package manager
    type: brewPackage
    shell: '/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"'
    doctorChecks:
      - type: commandExists
        name: Homebrew
        section: Dependencies
        command: brew

  - id: node
    displayName: Node.js
    description: JavaScript runtime (for npx-based MCP servers)
    dependencies: [homebrew]
    brew: node

  - id: gh
    displayName: GitHub CLI
    description: GitHub CLI for PR operations
    dependencies: [homebrew]
    brew: gh

  - id: jq
    description: JSON processor (used by hooks)
    dependencies: [homebrew]
    brew: jq

  - id: ollama
    displayName: Ollama
    description: Local LLM runtime (official installer, supports all Apple Silicon)
    type: configuration
    shell: "curl -fsSL https://ollama.com/install.sh | sh"
    doctorChecks:
      - type: commandExists
        name: Ollama
        section: Dependencies
        command: ollama

  - id: ollama-nomic-embed
    description: Pull nomic-embed-text model for docs-mcp-server embeddings
    type: configuration
    dependencies: [ollama]
    shell: "ollama pull nomic-embed-text"
    doctorChecks:
      - type: commandExists
        name: "nomic-embed-text model"
        section: AI Models
        command: ollama
        args: ["show", "nomic-embed-text"]

  # ── MCP Servers ─────────────────────────────────────────────────────────
  - id: docs-mcp-server
    description: Semantic search over memories using local Ollama embeddings
    dependencies: [node, ollama]
    mcp:
      command: npx
      args:
        - "-y"
        - "@arabold/docs-mcp-server@latest"
        - "--read-only"
        - "--telemetry=false"
      env:
        OPENAI_API_KEY: "ollama"
        OPENAI_API_BASE: "http://localhost:11434/v1"
        DOCS_MCP_EMBEDDING_MODEL: "openai:nomic-embed-text"

  # ── Skills ──────────────────────────────────────────────────────────────
  - id: skill-continuous-learning
    displayName: continuous-learning skill
    description: Extracts learnings and decisions from sessions into memory
    skill:
      source: skills/continuous-learning
      destination: continuous-learning

  # ── Hooks ───────────────────────────────────────────────────────────────
  - id: hook-continuous-learning
    displayName: Continuous learning activator
    description: Reminds to evaluate learnings on each prompt
    hookEvent: UserPromptSubmit
    hook:
      source: hooks/continuous-learning-activator.sh
      destination: continuous-learning-activator.sh

  - id: hook-sync-memories
    displayName: Sync memories hook
    description: Checks Ollama health and syncs docs-mcp-server library on session start
    dependencies: [ollama, docs-mcp-server, skill-continuous-learning, jq]
    hookEvent: SessionStart
    hook:
      source: hooks/sync-memories.sh
      destination: sync-memories.sh

  # ── Commands ────────────────────────────────────────────────────────────
  - id: command-pr
    displayName: /pr command
    description: Automates stage, commit, push, and PR creation with ticket extraction
    dependencies: [gh]
    command:
      source: commands/pr.md
      destination: pr.md

  # ── Configuration ───────────────────────────────────────────────────────
  - id: settings
    displayName: Settings
    description: Disables auto-memory in favor of continuous learning system
    isRequired: true
    settingsFile: config/settings.json

  - id: gitignore
    displayName: Global gitignore
    description: Ignores memory files from version control
    isRequired: true
    gitignore:
      - .claude/memories

# ---------------------------------------------------------------------------
# Templates — CLAUDE.local.md sections
# ---------------------------------------------------------------------------
templates:
  - sectionIdentifier: continuous-learning
    placeholders:
      - __PROJECT_DIR_NAME__
    contentFile: templates/continuous-learning.md

# ---------------------------------------------------------------------------
# Supplementary doctor checks
# ---------------------------------------------------------------------------
supplementaryDoctorChecks:
  - type: commandExists
    name: "Ollama service running"
    section: AI Models
    command: curl
    args: ["-sf", "http://localhost:11434/api/tags"]
    fixCommand: "open -a Ollama"
